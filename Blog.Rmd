---
title: "Qual data blog"
output: html_notebook
---

```{r, echo=FALSE, message=FALSE}
library(tm)
library(tidytext)
library(tidyverse)
library(SnowballC)
library(ggthemes)
library(wordcloud2)
library(topicmodels)
library(knitr)
```

#Introduction

Examples of the types of qualitative information that charities and not-for-profits might collect includes notes from their clients, service user feedback, or information about funders, amongst other examples. However, many charities may not necessarily be making the most of qualitative information, as, for many organisations, qualitative data can be more time-consuming than quantiative data to enter, verify, and analyse. Large-scale qualitative information furthermore tends to demand specials

This post looks at three approaches that NFPs can adopt to quickly analyse and draw insights from the qualitative data that they may possess, by leverging some data science tools and approaches.

#Approach one: Word frequency

The first and most basic approach is to look at the frequency breakdown of certain words in a given qualitative dataset. This is a simple approach which involves counting the frequency by which words occur. For example, below is a wordcloud of the 50 most frequent words used by Barack Obama in all of his State of the Union addresses (2010 - 2016). 

```{r, echo=FALSE}
data <- VCorpus(DirSource("Obama data"))

##Build the filterwords

filterwords <- c("ca","d0","d5","d3","d2","applause", "ve")

##Process the data

wordclouddata <- tidy(data) %>%
  mutate(Year = gsub(".txt","", id)) %>%
  unnest_tokens(word,text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!(word %in% filterwords)) %>%
  mutate(word = case_when(
    word=="americans" ~ "american",
    word=="jobs" ~ "job",
    word=="businesses" ~ "business",
    TRUE ~ word
  )) %>%
  count(word, sort = TRUE)

##Build the wordcloud
colorVec = rep(c("#28201C","#D9A638","#159A92","#9A2027"), length.out = nrow(new))
wordcloud2(wordclouddata, minSize=25, color= colorVec, shape = "circle", fontFamily = "sans")
```

The wordcloud allows us to quickly compare which words are most prevalant across a section of qualitative data by looking at the relative size of each of these words. In this case, the wordcloud shows us that Obama has mentioned 'America', 'job', and 'people' a lot in his SOTU addresses (as these are the biggest words) and has been less likely to use words like 'college', 'reform' and 'united' (the smallest words).

You can choose to get more sophistocated with word frequency approaches. In this case, becuase the data is organised by year, we can also graph the prevalence of particular words across different years, to see if any changes have occured. 

```{r, echo=FALSE, dpi=600, warning=FALSE, message=FALSE}
obamayears <- tidy(data) %>%
  mutate(Year = gsub(".txt","", id)) %>%
  unnest_tokens(word,text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!(word %in% filterwords)) %>%
  mutate(word = case_when(
    word=="americans" ~ "american",
    word=="jobs" ~ "job",
    word=="businesses" ~ "business",
    TRUE ~ word
  )) %>%
  count(word, Year, sort=TRUE) %>%
  group_by(Year) %>%
  top_n(20) %>%
  ungroup()

##Plot

ggplot(obamayears, aes(x = reorder(word, n), y= n, fill=Year)) +
  geom_col(show.legend=FALSE) +
  facet_wrap(~Year, scales = "free_y") +
  coord_flip() +
  xlab("") +
  ylab("")
```

Splitting word frequencies by the unit you're interested in (in the case of this example, the year of the SOTU address) can facilitate a quick compare and contrast.  In this case we can see that Obama and his speech writers have stuck to a relatively small group of words - indeed, only 21 different words make the top 10 between 2010-2016. Consistency is key!

In summation, a word frequency approach. Examples of ways that charities could use word frequency analyses include:

- See if there is any difference in the frequency of words used by different kinds of service users (e.g. long-term versus short-term users, or male and female service users) to describe their experiences;
- Compare and contrast the  (for example, over time, or at different geographic locations).

#Approach two: Sentiment analysis

Another approach to qualitative analysis is sentiment analysis. A sentiment analysis looks at the emotional meaning of words, by classifing each word into a pre-defined sentiment category. For example, the word 'encourage' might be coded as 'positive' in sentiment, whilst the word 'condescend' would probably be coded as 'negative'. 

There are different approaches to classifying sentiments. Some dictionaries score words from positive to negative. Others take a binary approach, classifying words into particular sentiments, or simply as 'positive' or 'negative'. For this example, I'm going to make use of the `nrc` dictionary from the `R tidytext` package, which classifies individual words into one of 10 different categories, e.g. 'angry', 'fear', 'joy', 'trust', and 'sadness'.  

```{r, echo=FALSE, warnings= FALSE, message = FALSE}
politics <- read_csv("https://raw.githubusercontent.com/bsuthersan/QualitativeData/master/Politics/politics.csv")

#Remove the mod

politics <- politics %>%
  filter(!str_detect(comment, "As a reminder, this subreddit"))
```

For this example, I've scraped a small amount of comment data from Reddit, from the subreddit /r/politics. This is a popular subreddit which discusses a number of political topics. The data comes from topics discussed on the 1st and 2nd of August 2018, and comprises a total of `r length(politics$title)` comments on `r n_distinct(politics$title)` different topics, from `r n_distinct(politics$user)` different Reddit users (or 'Redditors').

```{r, echo=FALSE, message=FALSE, warning=FALSE}
politicsdata <- politics %>%
  select(title, post_date, comment) %>%
  unnest_tokens(word, comment) %>%
  anti_join(stop_words, by = "word") %>%
  left_join(nrc, by = "word") %>%
  filter(!is.na(sentiment))
```

First, let's graph the prevalence of different sentiments across all of the comments in /r/politics.

```{r, echo=FALSE, message=FALSE, warning=FALSE, dpi=600}
##Basic sentiment analysis

positive = c("positive", "trust","joy")
neutral = c('surprise','anticipation')

politicsdata %>%
group_by(sentiment) %>%
  summarise(Total = n()) %>%
  mutate(Percent = Total/sum(Total)*100) %>%
  ggplot(aes(reorder(sentiment,Percent), Percent)) +
  geom_col(fill="steelblue") +
  coord_flip() +
  xlab("") + 
  ylab("Percent of words") +
  theme_minimal() +
  theme(legend.title = element_blank())
```

Above, we can see that the most prevalent sentiments associated with words were 'positive', 'negative' and 'trust' respectively. Interestingly, however, nearly 1 in 5 topics were associated with anger and fear sentiments.  

Again, like a word frequency analysis, you can choose to get more sophistocated with a sentiment analysis. For example, using a separate dictionary 'afinn', which scores words on a continum from positive (+5) to negative (-5), we can determine which 10 posts are associated with the most negaitve sentiments, by looking at the average scores of all the words in associated comments. (Note that to avoid skewing, I've only included posts with 100 distinc words or more.)

```{r, echo=FALSE}
afinn <- get_sentiments("afinn")
politicsdata %>%
  select(-sentiment) %>%
  left_join(afinn, by = "word") %>%
  filter(!is.na(score)) %>%
  group_by(title) %>%
  summarise(SentimentScore = round(mean(score),1),
            DistinctWords = n_distinct(word)) %>%
  filter(DistinctWords > 100) %>%
  select(-DistinctWords) %>%
  arrange(SentimentScore) %>%
  top_n(-10) %>%
  kable()
```

Many of these topics do indeed appear to be quite negative in tone. 

A sentiment analysis builds on a word frequency analysis; it helps us to determine which the emotions and sentinments in words.

- You could use a sentiment analysis to quickly identify which comments or sentences in a given document are more positive or negative in tone. This could be useful, if, for example, you are looking to quickly identify a positive or negative quote for a report;
- You can use sentinment analyses to pick up differences between different locations, individuals, etc.

##Topic modelling

Topic modelling is a techinque commonly used in machine learning and natural langauge processing. Basically, through topic modelling you can attempt to 'discover' the different topics or themes that are prevalent in your textual data. You can determine the topics by looking at the words that are associated with each topic.

There are a number of different algorithms used for topic modelling. One of the most popular algorithms is the Latent Dirichlet allocation (LDA), which treats each 

Keeping with our politics theme, for this example I have accessed the tweets of Donald Trump, using the [trumptwitterarchive]("http://www.trumptwitterarchive.com/"), which, as the same suggests, is an online archive of all of Donald Trump's tweets from his handle `@realDonaldTrump`. To reduce the data, I've just pulled all tweets from the last three years.


```{r, echo=FALSE, message = FALSE, warning = FALSE}
##Pull the data, and turn into document term matrix
url <- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'
removewords <- c("realdonaldtrump","trump","https", "t.co","donald", "amp","http", "rt")
trump_tweets <- map(2017:2018, ~sprintf(url, .x)) %>%
  map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
  select(text, created_at) %>%
  unnest_tokens(word,text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!(word %in% removewords)) %>%
  group_by(word, created_at) %>%
  summarise(total = n())
trumpdtm <- cast_dtm(trump_tweets, created_at, word, total)
```

Once the data has been collected, we can apply the LDA algorithm to it.

```{r, echo=FALSE}
trump_topics <- LDA(trumpdtm, k = 2, control = list(seed = 1234))
trump_topics_lda <- tidy(trump_topics)
 trump_topics_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(term = reorder(term, beta),
         topic  = as.factor(topic)) %>%
  ggplot(aes(term, beta, fill=topic)) +
   geom_bar(stat='identity', labs=FALSE) +
   guides(fill=FALSE) +
   facet_wrap(~topic, scales='free') +
   theme_minimal() +
   coord_flip() +
   xlab("")
```



















